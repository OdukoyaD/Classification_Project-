{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49acec5",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "* Feature engineering is the process of extracting features from raw data using domain expertise and data mining techniques.\n",
    "* We must first clean and reshape the data before we can use it to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a846ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\thape\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\thape\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\thape\\anaconda3\\lib\\site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\thape\\anaconda3\\lib\\site-packages (from scipy) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "!pip install scikit-learn\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50030bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages for general processing of data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "# set plot style\n",
    "sns.set()\n",
    "\n",
    "#packages for natural language processing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string\n",
    "\n",
    "#Packages for machine learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#packages for checking the performance of the models used\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#packages for web scraping\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "#packages for balancing our data\n",
    "from sklearn.utils import resample\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2e559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./train.csv') # This code imports the train data \n",
    "df_test = pd.read_csv('./test.csv') # This code import the test data\n",
    "sample = pd.read_csv('./sample_submission.csv') #This code imports a sample submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0caff47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.copy() # This code makes a copy of the df_train\n",
    "test_data = df_test.copy() # This code makes a copy of the df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46e52db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message    0\n",
       "tweetid    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for this purpose, we combine both the test and train data set\n",
    "#we drop the sentiment feature for compatablity reasons\n",
    "df_all = pd.concat([df.drop('sentiment', axis = 1),test_data]) \n",
    "\n",
    "\n",
    "#we then check for null values in both data sets\n",
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aa8ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.message = df_all.message.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75edbd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing punctuations\n",
      "extracting information from links\n",
      "removing links\n",
      "applying lematization\n"
     ]
    }
   ],
   "source": [
    "df_all.message = df_all.message.str.lower()\n",
    "\n",
    "#funtions that will perform several tasks\n",
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n",
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def extract(m):\n",
    "    for i in m:\n",
    "        if i[:4] in 'http':\n",
    "            try:\n",
    "                html = urlopen(i)\n",
    "            except:\n",
    "                html = 'url_web'\n",
    "                soup = BeautifulSoup(html)\n",
    "            try:\n",
    "                title = soup.title.text\n",
    "            except AttributeError:\n",
    "                title = 'url_web'\n",
    "            except:\n",
    "                title = 'url_web'\n",
    "            m.append(title.split())\n",
    "\n",
    "            \n",
    "#we remove punctuations\n",
    "print('removing punctuations')\n",
    "df_all['message'] = df_all['message'].apply(remove_punctuation)\n",
    "df_all['message'] = df_all['message'].apply(lambda x:x.split())\n",
    "\n",
    "\n",
    "#we extract information from the links\n",
    "print('extracting information from links')\n",
    "#df_all['message'] = df_all['message'].apply(lambda x: extract(x))           \n",
    "                       \n",
    "#we remove any uncaught links           \n",
    "print('removing links')\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r''\n",
    "df_all['message'] = df_all['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "\n",
    "#we apply lematizers\n",
    "print('applying lematization')\n",
    "df_all['message'] = df_all['message'].apply(mbti_lemma, args=(lemmatizer, ))\n",
    "df_all['message'] = df_all['message'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8585124",
   "metadata": {},
   "source": [
    "We then transform the data into features for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "087438b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape is (15819, 8001)\n",
      "testind data shape is (10546, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Let's use the count vectorizer with its default hyperparameters\n",
    "vect = CountVectorizer(lowercase=True,max_features=8000, stop_words='english',analyzer='word', ngram_range=(1, 3), min_df=4, max_df=0.5)\n",
    "X_count = vect.fit_transform(df_all['message'].values.astype(str))\n",
    "\n",
    "#we devide our dataset back to the training and testing set\n",
    "train = X_count[:15819]\n",
    "test = X_count[15819:]\n",
    "\n",
    "#we then devide our data into x and y for machine learning algorithm\n",
    "warnings.filterwarnings('ignore')\n",
    "X = train.toarray()\n",
    "train = pd.DataFrame(X, columns = vect.get_feature_names())\n",
    "y = df.sentiment\n",
    "train['y'] = y\n",
    "\n",
    "\n",
    "#we print the shape of our training and testing data to ensure that we are good\n",
    "print(f'training data shape is {train.shape}')\n",
    "print(f'testind data shape is {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d72a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
